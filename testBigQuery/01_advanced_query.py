from google.cloud import bigquery

# åˆå§‹åŒ–å®¢æˆ·ç«¯
# æ›¿æ¢ä¸ºä½ çš„é¡¹ç›® ID
PROJECT_ID = "webeye-internal-test"
client = bigquery.Client(project=PROJECT_ID)


def run_parameterized_query(state_name, limit_count):
    """
    è¿è¡Œå‚æ•°åŒ–æŸ¥è¯¢ã€‚
    å‚æ•°åŒ–æŸ¥è¯¢å¯ä»¥é˜²æ­¢ SQL æ³¨å…¥ï¼Œå¹¶ä¸”å…è®¸ BigQuery ç¼“å­˜æŸ¥è¯¢è®¡åˆ’ï¼Œæé«˜æ•ˆç‡ã€‚
    """

    # 1. å®šä¹‰ SQLï¼Œä½¿ç”¨ @ç¬¦å· å®šä¹‰å‚æ•°å ä½ç¬¦
    # ğŸ’¡ æœ€ä½³å®è·µ: æ°¸è¿œä½¿ç”¨å‚æ•°åŒ–æŸ¥è¯¢ï¼Œå³ä½¿æ˜¯å†…éƒ¨ç³»ç»Ÿã€‚
    # å®ƒå¯ä»¥é˜²æ­¢ SQL æ³¨å…¥ï¼Œå¹¶ä¸” BigQuery å¯ä»¥ç¼“å­˜ç¼–è¯‘åçš„æŸ¥è¯¢è®¡åˆ’ï¼Œå¤ç”¨æ€§æ›´é«˜ã€‚
    query = """
        SELECT name, SUM(number) as total_count
        FROM `bigquery-public-data.usa_names.usa_1910_current`
        WHERE state = @state
        GROUP BY name
        ORDER BY total_count DESC
        LIMIT @limit
    """

    # 2. é…ç½®æŸ¥è¯¢å‚æ•°
    job_config = bigquery.QueryJobConfig(
        query_parameters=[
            # æ ‡é‡å‚æ•° (Scalar)
            bigquery.ScalarQueryParameter("state", "STRING", state_name),
            bigquery.ScalarQueryParameter("limit", "INT64", limit_count),
        ]
    )

    print(f"æ­£åœ¨æŸ¥è¯¢å·: {state_name}, é™åˆ¶: {limit_count}...")

    # 3. è¿è¡ŒæŸ¥è¯¢ (å¸¦æœ‰é…ç½®)
    query_job = client.query(query, job_config=job_config)

    # 4. è·å–ç»“æœ (ä½¿ç”¨ Storage API åŠ é€Ÿï¼Œå› ä¸ºæˆ‘ä»¬åˆšæ‰å®‰è£…äº†ä¾èµ–)
    # ğŸ’¡ æ€§èƒ½æç¤º: to_dataframe() é»˜è®¤å°è¯•ä½¿ç”¨ BigQuery Storage API (äºŒè¿›åˆ¶åè®®)ã€‚
    # ç›¸æ¯”ä¼ ç»Ÿçš„ JSON REST APIï¼Œä¸‹è½½å¤§ç»“æœé›†æ—¶é€Ÿåº¦å¿«éå¸¸å¤šã€‚
    # ç°åœ¨çš„ to_dataframe ä¼šè‡ªåŠ¨å°è¯•ä½¿ç”¨ google-cloud-bigquery-storage
    df = query_job.to_dataframe()

    print("\næŸ¥è¯¢ç»“æœ (Top Rows):")
    print(df.head())

    return df


if __name__ == "__main__":
    # å°è¯•æŸ¥è¯¢ 'CA' (åŠ å·) çš„å‰ 5 å
    run_parameterized_query("CA", 5)
