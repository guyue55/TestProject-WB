from google.cloud import bigquery
from google.cloud.exceptions import NotFound
import datetime

PROJECT_ID = "webeye-internal-test"
DATASET_ID = f"{PROJECT_ID}.learning_bq"
# æˆ‘ä»¬å°†åˆ›å»ºä¸€ä¸ªç”¨æ¥å­˜å‚¨æ—¥å¿—çš„æ–°è¡¨
TABLE_ID = f"{DATASET_ID}.app_logs"

client = bigquery.Client(project=PROJECT_ID)

def create_partitioned_clustered_table():
    """
    åˆ›å»ºä¸€ä¸ªæ—¢æœ‰åˆ†åŒº(Partitioning)åˆæœ‰åˆ†ç°‡(Clustering)çš„è¡¨ã€‚
    è¿™æ˜¯ BigQuery æ€§èƒ½ä¼˜åŒ–çš„é»„é‡‘ç»„åˆã€‚
    """
    print(f"--- æ­£åœ¨åˆ›å»ºåˆ†åŒº+åˆ†ç°‡è¡¨: {TABLE_ID} ---")
    
    # 1. å®šä¹‰ Schema
    schema = [
        bigquery.SchemaField("log_id", "STRING", mode="REQUIRED"),
        bigquery.SchemaField("event_timestamp", "TIMESTAMP", mode="REQUIRED"), # åˆ†åŒºå­—æ®µ
        bigquery.SchemaField("user_id", "INTEGER", mode="NULLABLE"),           # åˆ†ç°‡å­—æ®µ
        bigquery.SchemaField("event_type", "STRING", mode="NULLABLE"),         # åˆ†ç°‡å­—æ®µ
        bigquery.SchemaField("message", "STRING", mode="NULLABLE"),
    ]

    table = bigquery.Table(TABLE_ID, schema=schema)

    # 2. é…ç½®åˆ†åŒº (Partitioning)
    # æŒ‰å¤©åˆ†åŒºã€‚æŸ¥è¯¢æ—¶å¦‚æœå¸¦ä¸Š WHERE date(event_timestamp) = ... ä¼šæå¤§å‡å°‘æ‰«æé‡
    table.time_partitioning = bigquery.TimePartitioning(
        type_=bigquery.TimePartitioningType.DAY,
        field="event_timestamp",  # æŒ‡å®šåˆ†åŒºåˆ—
        expiration_ms=None,       # æ•°æ®æ°¸ä¸è¿‡æœŸ (å¯ä»¥è®¾ç½®ä¾‹å¦‚ 30å¤©åè‡ªåŠ¨åˆ é™¤)
    )

    # 3. é…ç½®åˆ†ç°‡ (Clustering)
    # åœ¨åŒä¸€ä¸ªåˆ†åŒºå†…ï¼Œæ•°æ®ä¼šæ ¹æ®è¿™äº›å­—æ®µæ’åºã€‚
    # æŸ¥è¯¢ WHERE user_id = 123 æ—¶ï¼ŒBigQuery å¯ä»¥ç›´æ¥è·³åˆ°ç›¸å…³çš„æ•°æ®å—ï¼Œé¿å…å…¨åˆ†åŒºæ‰«æã€‚
    table.clustering_fields = ["user_id", "event_type"]

    try:
        table = client.create_table(table)
        print(f"è¡¨åˆ›å»ºæˆåŠŸ: {table.full_table_id}")
        print(f"åˆ†åŒºç±»å‹: {table.time_partitioning.type_}")
        print(f"åˆ†ç°‡å­—æ®µ: {table.clustering_fields}")
    except Exception as e:
        print(f"è¡¨å¯èƒ½å·²å­˜åœ¨æˆ–å‡ºé”™: {e}")

def insert_data_into_specific_partition():
    """æ¼”ç¤ºæ’å…¥æ•°æ®"""
    print("\n--- æ’å…¥ç¤ºä¾‹æ•°æ® ---")
    now = datetime.datetime.now()
    rows = [
        {"log_id": "L1", "event_timestamp": str(now), "user_id": 1001, "event_type": "login", "message": "User logged in"},
        {"log_id": "L2", "event_timestamp": str(now), "user_id": 1002, "event_type": "logout", "message": "User logged out"},
        # æ•…æ„æ’å…¥ä¸€æ¡æ˜¨å¤©çš„æ•°æ® (ä¼šè‡ªåŠ¨è½å…¥æ˜¨å¤©çš„åˆ†åŒº)
        {"log_id": "L3", "event_timestamp": str(now - datetime.timedelta(days=1)), "user_id": 1001, "event_type": "click", "message": "Late arriving data"},
    ]
    
    errors = client.insert_rows_json(TABLE_ID, rows)
    if not errors:
        print("æ•°æ®æ’å…¥æˆåŠŸï¼")
    else:
        print(f"æ’å…¥é”™è¯¯: {errors}")

def query_optimized():
    """æ¼”ç¤ºå¦‚ä½•åˆ©ç”¨åˆ†åŒºæŸ¥è¯¢ (Pruning)"""
    print("\n--- æ¼”ç¤ºä¼˜åŒ–æŸ¥è¯¢ ---")
    # æ³¨æ„ WHERE å­å¥å¿…é¡»åŒ…å«åˆ†åŒºåˆ—ï¼Œæ‰èƒ½äº§ç”Ÿè£å‰ªæ•ˆæœ(Pruning)
    query = f"""
        SELECT *
        FROM `{TABLE_ID}`
        WHERE date(event_timestamp) = CURRENT_DATE() 
          AND user_id = 1001
    """
    
    # ğŸ’¡ æ ¸å¿ƒåŸç†: Partition Pruning (åˆ†åŒºè£å‰ª)
    # BigQuery çœ‹åˆ° WHERE date(...) æ¡ä»¶åï¼Œä¼šç›´æ¥å¿½ç•¥æ‰æ˜¨å¤©ã€å‰å¤©ç­‰æ‰€æœ‰ä¸åŒ¹é…çš„åˆ†åŒºæ–‡ä»¶ã€‚
    # è¿™å°±æ˜¯ä¸ºä»€ä¹ˆåˆ†åŒºè¡¨èƒ½çœé’±çš„åŸå› ã€‚å¦‚æœæ²¡æœ‰è¿™ä¸ª WHERE æ¡ä»¶ï¼Œå®ƒä¼šæ‰«æå…¨è¡¨ï¼
    
    job_config = bigquery.QueryJobConfig(dry_run=True)
    query_job = client.query(query, job_config=job_config)
    print(f"æ­¤æŸ¥è¯¢å°†æ‰«æå­—èŠ‚æ•°: {query_job.total_bytes_processed} (å¦‚æœè¡¨å¾ˆå¤§ï¼Œè¿™ä¸ªæ•°å­—ä¼šè¿œå°äºå…¨è¡¨æ‰«æ)")

if __name__ == "__main__":
    # å¦‚æœè¡¨å·²å­˜åœ¨å¦‚æœè¦é‡æ–°æ¼”ç¤ºï¼Œå»ºè®®å…ˆå»æ§åˆ¶å°åˆ æ‰æˆ–è€…ä¿®æ”¹ä»£ç é€»è¾‘
    # client.delete_table(TABLE_ID, not_found_ok=True) 
    
    create_partitioned_clustered_table()
    insert_data_into_specific_partition()
    query_optimized()
